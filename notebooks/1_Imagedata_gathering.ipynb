{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9abe7d2a-8c58-41d7-9038-be9de42e134b",
   "metadata": {},
   "source": [
    "# Real-Time Dangerous Animal Detection Using Big Data and Deep Learning with YOLO"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48c3575e-a246-4137-ae19-1585bf7c1477",
   "metadata": {},
   "source": [
    "- This project aims to develop a real-time dangerous animal detection system using YOLO (You Only Look Once) with deep learning and big data processing techniques. The system is designed to enhance public safety by detecting potentially hazardous animals such as snakes, scorpions, and wolves through camera feeds. The dataset, consisting of both real-world and augmented images, is preprocessed using PySpark to ensure consistency in format, resolution, and quality. Various augmentation techniques, including motion blur, are applied to improve performance in dynamic environments. The model is trained with YOLOv8 to enhance detection accuracy under different conditions."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98533282-d88e-42d5-bc47-3282030e4bae",
   "metadata": {},
   "source": [
    "### `Step-1` : Gathering Dataset\n",
    "- The dataset includes eight selected dangerous animal classes that are commonly seen in public areas: bear, crocodile, hawk, lion, scorpion, snake, spider, and wolf. To gather the dataset, images were sourced from `Google Images`, `Pexels`, and `Unsplash`. For Google Images, the GoogleImageCrawler library was used to automate the downloading process. For Pexels and Unsplash, a developer account was created to obtain an API key for downloading images. Approximately 3,000 images were collected in total."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c8d70c9b-b357-4138-909c-bcaeb1a7e530",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import os\n",
    "import requests\n",
    "import time\n",
    "from icrawler.builtin import GoogleImageCrawler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "019b414f-7f8d-4150-90ea-c74542a21024",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pip install icrawler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a1e90c0b-feb5-4904-a6c2-7d9d27244613",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pip install pytesseract"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "44b52d4b-74c2-44e8-bf33-e20d2ff5a32f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pip install opencv-python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1a45330e-c750-45ec-a8b1-fa6c2dc1d217",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pip install requests\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "926d57f1-a224-408e-aee7-59a681c879a5",
   "metadata": {},
   "source": [
    "### The Images from Google Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8093224-8a1b-41ff-8732-8ab6197f5b33",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the folder and animal classes\n",
    "animal_classes = ['snake', 'scorpion', 'spider', 'crocodile', 'wolf', 'bear', 'lion', 'hawk']  # List of animal classes\n",
    "download_folder = 'google_image'  # Name of the main folder where images will be saved\n",
    "\n",
    "# Function to download images\n",
    "def download_images():\n",
    "    # Create main google_image folder if not exists\n",
    "    os.makedirs(download_folder, exist_ok=True)  # Creates the root folder 'google_image' if it doesn't exist\n",
    "\n",
    "    # Loop through each animal class in the list\n",
    "    for animal in animal_classes:\n",
    "        print(f\"üîç Downloading images for: {animal}\")  # Print a message to indicate which animal's images are being downloaded\n",
    "        \n",
    "        # Create subfolder for each animal class within google_image folder\n",
    "        animal_folder = os.path.join(download_folder, animal)  # Define the path for the animal-specific subfolder\n",
    "        os.makedirs(animal_folder, exist_ok=True)  # Create the folder for the current animal if it doesn't exist\n",
    "\n",
    "        # Initialize GoogleImageCrawler\n",
    "        google_crawler = GoogleImageCrawler(storage={'root_dir': animal_folder})  # Set up the image crawler for the current animal folder\n",
    "\n",
    "        # Download images using the GoogleImageCrawler\n",
    "        google_crawler.crawl(\n",
    "            keyword=f\"{animal} in natural habitat\",  # The search keyword will be the animal in its natural habitat\n",
    "            max_num=images_per_class,  # Limit the number of images to download per animal\n",
    "            file_idx_offset='auto'  # Automatically assign file indices for the images\n",
    "        )\n",
    "\n",
    "        # Short delay to ensure system releases file locks\n",
    "        time.sleep(5)  # Add a small delay to avoid overloading the system and ensure file operations are properly finished\n",
    "\n",
    "        print(f\"‚úÖ Download complete for: {animal}\")  # Print a message when the download is complete for the current animal\n",
    "\n",
    "# Run the function to start downloading images\n",
    "download_images()  # Call the function to start the image downloading process\n",
    "\n",
    "print(\"‚úÖ All image downloads complete!\")  # Print a final message indicating that all image downloads are done\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e9469b1-9ba2-48cf-8bff-8fede0e619b0",
   "metadata": {},
   "source": [
    "### The Images from Pexels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad9f3c13-204e-40d1-b692-370aa37ee4ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pexels API Key (replace with your own key)\n",
    "PEXELS_API_KEY = '**************'\n",
    "\n",
    "# Animal classes to download images for\n",
    "animal_classes = ['snake', 'scorpion', 'spider', 'crocodile', 'wolf', 'bear', 'lion', 'hawk']\n",
    "\n",
    "# Function to download images from Pexels\n",
    "def download_pexels_images(query, num_images=500, parent_dir=\"pexels_images\"):\n",
    "    headers = {'Authorization': PEXELS_API_KEY}  # API authorization header\n",
    "\n",
    "    # Create parent directory and class-specific folder\n",
    "    os.makedirs(parent_dir, exist_ok=True)\n",
    "    animal_folder = os.path.join(parent_dir, query)\n",
    "    os.makedirs(animal_folder, exist_ok=True)\n",
    "\n",
    "    # Variables for pagination and tracking downloaded images\n",
    "    per_page = 80  # Pexels API limit per page\n",
    "    total_images_downloaded = 0\n",
    "    page = 1\n",
    "\n",
    "    # Download loop until desired number of images is reached\n",
    "    while total_images_downloaded < num_images:\n",
    "        url = f'https://api.pexels.com/v1/search?query={query}&per_page={per_page}&page={page}'\n",
    "        response = requests.get(url, headers=headers)\n",
    "\n",
    "        if response.status_code == 200:\n",
    "            data = response.json()\n",
    "            if not data['photos']:  # Stop if no more images found\n",
    "                print(\"No more images found!\")\n",
    "                break\n",
    "\n",
    "            # Download images and save with original names\n",
    "            for photo in data['photos']:\n",
    "                img_url = photo['src']['original']\n",
    "                img_path = os.path.join(animal_folder, os.path.basename(img_url))\n",
    "\n",
    "                try:\n",
    "                    img_data = requests.get(img_url).content\n",
    "                    with open(img_path, 'wb') as f:\n",
    "                        f.write(img_data)\n",
    "                    total_images_downloaded += 1\n",
    "                    print(f\"Downloaded {img_path}\")\n",
    "\n",
    "                    # Stop once target number of images is reached\n",
    "                    if total_images_downloaded >= num_images:\n",
    "                        break\n",
    "                except Exception as e:\n",
    "                    print(f\"Error downloading {img_path}: {e}\")\n",
    "\n",
    "            # Move to the next page if needed\n",
    "            page += 1\n",
    "        else:\n",
    "            print(f\"Error fetching data for {query} from Pexels API\")\n",
    "            break\n",
    "\n",
    "# Download 500 images for each animal class\n",
    "for animal in animal_classes:\n",
    "    download_pexels_images(animal, num_images=500, parent_dir=\"pexels_images\")\n",
    "\n",
    "print(\"Download completed for all animal classes!\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "efaccf58-3c02-4476-8257-ceb83264babb",
   "metadata": {},
   "source": [
    "### The Images from Unsplash"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c887ab52-db1a-4bc3-94cd-b126bb36c6c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# List of dangerous animal classes\n",
    "animal_classes = ['snake', 'scorpion', 'spider', 'crocodile', 'wolf', 'bear', 'lion', 'hawk']\n",
    "\n",
    "# Function to download images from Unsplash\n",
    "def download_images(query, client_id, download_path, num_images=500):\n",
    "    if not os.path.exists(download_path):\n",
    "        os.makedirs(download_path)\n",
    "\n",
    "    url = \"https://api.unsplash.com/search/photos\"\n",
    "    params = {\n",
    "        \"query\": query,\n",
    "        \"client_id\": client_id,\n",
    "        \"per_page\": 30  # Unsplash API limit per request\n",
    "    }\n",
    "\n",
    "    total_downloaded = 0\n",
    "    page = 1\n",
    "\n",
    "    # Continue downloading until reaching desired image count\n",
    "    while total_downloaded < num_images:\n",
    "        params[\"page\"] = page\n",
    "        response = requests.get(url, params=params)\n",
    "\n",
    "        if response.status_code == 200:\n",
    "            data = response.json()\n",
    "            if not data['results']:\n",
    "                print(f\"No more images found for {query}\")\n",
    "                break\n",
    "\n",
    "            # Download and save images\n",
    "            for idx, photo in enumerate(data['results']):\n",
    "                if total_downloaded >= num_images:\n",
    "                    break\n",
    "                image_url = photo['urls']['regular']\n",
    "                image_response = requests.get(image_url)\n",
    "                if image_response.status_code == 200:\n",
    "                    with open(os.path.join(download_path, f\"{query}_{total_downloaded + 1}.jpg\"), 'wb') as file:\n",
    "                        file.write(image_response.content)\n",
    "                    print(f\"Downloaded {query}_{total_downloaded + 1}.jpg\")\n",
    "                    total_downloaded += 1\n",
    "                else:\n",
    "                    print(f\"Failed to download image {total_downloaded + 1}\")\n",
    "        else:\n",
    "            print(f\"Error fetching data from Unsplash API: {response.status_code}\")\n",
    "            break\n",
    "\n",
    "        page += 1\n",
    "\n",
    "# Example usage\n",
    "if __name__ == \"__main__\":\n",
    "    CLIENT_ID = \"**********\"  # Replace with your Unsplash API key\n",
    "    base_path = \"./unsplash\"\n",
    "\n",
    "    for animal in animal_classes:\n",
    "        download_images(animal, CLIENT_ID, os.path.join(base_path, animal), num_images=500)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
